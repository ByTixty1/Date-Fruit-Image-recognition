{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVAHOjXtNf37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    epochs = 7\n",
        "    lr = 0.001\n"
      ],
      "metadata": {
        "id": "aLP3XUbjNrpB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7z0D1s430svP"
      },
      "outputs": [],
      "source": [
        "#################################\n",
        "# efficientnet_b0\n",
        "#################################\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "class DateClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=9 , pretrained=True):\n",
        "        super(DateClassifier, self).__init__()\n",
        "        self.model = torchvision.models.efficientnet_b0(pretrained=pretrained)\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Linear(self.model.classifier[1].in_features, 512),\n",
        "            nn.BatchNorm1d(512),  # Batch Normalization added here\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes),\n",
        "            nn.BatchNorm1d(num_classes),  # Batch Normalization added here\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio wikipedia\n",
        "\n",
        "import gradio as gr\n",
        "import wikipedia\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the classes (assuming these are the same as used in your model)\n",
        "classes = ['Ajwa', 'Galaxy', 'Medjool', 'Meneifi', 'Nabtat Ali', 'Rutab', 'Shaishe', 'Sokari', 'Sugaey']\n",
        "\n",
        "# Load the saved model state dictionary\n",
        "# Ensure 'best_model.pth' exists and is the path to your trained model\n",
        "model_path = 'best_model (2).pth'\n",
        "try:\n",
        "    # Create a new instance of the model architecture\n",
        "    model = DateClassifier(num_classes=len(classes), pretrained=False).to(CFG.device) # Set pretrained=False when loading weights\n",
        "\n",
        "    # Load the state dictionary\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device(CFG.device)))\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    print(f\"Model loaded successfully from {model_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Model file not found at {model_path}. Please ensure the training and saving process completed successfully.\")\n",
        "    model = None # Set model to None if loading fails\n",
        "\n",
        "# Define the image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    # Add normalization if your training data was normalized\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def classify_date_and_get_description(image):\n",
        "    if model is None:\n",
        "        return \"Model not loaded.\", \"Please ensure the model file exists and is correctly loaded.\"\n",
        "\n",
        "    try:\n",
        "        # Convert the Gradio Image to a PIL Image if needed\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
        "\n",
        "        # Apply transformations\n",
        "        image = transform(image).unsqueeze(0).to(CFG.device) # Add batch dimension\n",
        "\n",
        "        # Make a prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image)\n",
        "            _, predicted_idx = torch.max(outputs, 1)\n",
        "            predicted_class = classes[predicted_idx.item()]\n",
        "\n",
        "        # Get description from Wikipedia\n",
        "        try:\n",
        "            # Search Wikipedia for the date type\n",
        "            page = wikipedia.page(f\"{predicted_class} date\", auto_suggest=True)\n",
        "            description = page.summary\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            description = f\"Wikipedia disambiguation for '{predicted_class} date': {e}\"\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            description = f\"Could not find a specific Wikipedia page for '{predicted_class} date'. Searching for just '{predicted_class}'.\"\n",
        "            try:\n",
        "                 page = wikipedia.page(f\"{predicted_class}\", auto_suggest=True)\n",
        "                 description = page.summary\n",
        "            except:\n",
        "                 description = f\"Could not find a relevant Wikipedia page for '{predicted_class}'.\"\n",
        "        except Exception as e:\n",
        "            description = f\"An error occurred while fetching Wikipedia information: {e}\"\n",
        "\n",
        "\n",
        "        return predicted_class, description\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during processing: {e}\", \"Could not classify the image or fetch description.\"\n",
        "\n",
        "\n",
        "# Create the Gradio interface\n",
        "if model is not None:\n",
        "    interface = gr.Interface(\n",
        "        fn=classify_date_and_get_description,\n",
        "        inputs=gr.Image(type=\"numpy\", label=\"Upload Date Image\"),\n",
        "        outputs=[\n",
        "            gr.Textbox(label=\"Date Type\"),\n",
        "            gr.Textbox(label=\"Description (from Wikipedia)\")\n",
        "        ],\n",
        "        title=\"Date Fruit Classifier\",\n",
        "        description=\"Upload an image of a date fruit and I'll tell you its type and provide a brief description from Wikipedia.\"\n",
        "    )\n",
        "\n",
        "    # Launch the interface\n",
        "    interface.launch(debug=True)\n",
        "else:\n",
        "    print(\"Gradio interface cannot be launched because the model failed to load.\")\n"
      ],
      "metadata": {
        "id": "XM_e0MKrhUSe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "outputId": "e8d0f129-1883-4d8d-e470-6502f1475c9e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from best_model (2).pth\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://57cfb8d91eb760aadd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://57cfb8d91eb760aadd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://154c7480472ebf23dc.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://57cfb8d91eb760aadd.gradio.live\n"
          ]
        }
      ]
    }
  ]
}